# v0.9.0 - Optimized

## Overview

This release begins the Performance Phase by profiling and optimizing hot paths. The focus is on improving runtime performance, reducing memory usage, and cleaning up dead code.

**Phase**: Performance
**Duration**: 2 weeks
**Prerequisites**: v0.8.0 (process safety complete, Robustness Phase done)

## Objectives

1. Profile critical hot paths
2. Optimize neuron forward propagation
3. Optimize weight perturbation and mutation
4. Optimize Mnesia database access
5. Remove dead code and unused fields
6. Add performance benchmarks

## Profiling and Optimization

### 1. Identify Hot Paths

**Expected Hot Paths**:
- Neuron forward propagation (signal processing)
- Weight perturbation during tuning
- Fitness calculation and aggregation
- Mnesia reads during mutation

**Profiling Approach**:
```erlang
%% Profile with fprof
profile_evolution() ->
    fprof:trace(start),
    population_monitor:run_evolution(TestPopulation),
    fprof:trace(stop),
    fprof:profile(),
    fprof:analyse([{dest, "evolution_profile.txt"}]).

%% Profile specific function
profile_neuron_forward() ->
    eprof:start(),
    eprof:start_profiling([self()]),
    [neuron_forward_test() || _ <- lists:seq(1, 10000)],
    eprof:stop_profiling(),
    eprof:analyze().
```

### 2. Optimize Neuron Forward Propagation

**Current Bottlenecks**:
- List operations in signal aggregation
- Weight tuple unpacking
- Activation function calls

**Optimizations**:

```erlang
%% Optimization 1: Use list comprehension instead of recursion
%% Before
dot_product_old([], [], Acc) -> Acc;
dot_product_old([{W,_,_,_}|Ws], [I|Is], Acc) ->
    dot_product_old(Ws, Is, Acc + W * I).

%% After (2-3x faster for large lists)
dot_product(Weights, Inputs) ->
    lists:sum([W * I || {{W,_,_,_}, I} <- lists:zip(Weights, Inputs)]).

%% Optimization 2: Pre-extract weights for repeated use
%% Before: unpack tuple every time
%% After: extract weights once during initialization
prepare_weights(WeightSpecs) ->
    [W || {W, _, _, _} <- WeightSpecs].

%% Use extracted weights for computation
dot_product_fast(Weights, Inputs) ->
    lists:sum([W * I || {W, I} <- lists:zip(Weights, Inputs)]).
```

**Activation Function Optimization**:
```erlang
%% Use NIF for expensive math (if needed)
%% Or use lookup tables for approximation

%% Cache frequently used values
-define(TANH_CACHE_SIZE, 1000).

init_tanh_cache() ->
    Cache = ets:new(tanh_cache, [set, public, named_table]),
    [ets:insert(Cache, {X, math:tanh(X)})
     || X <- generate_cache_keys(?TANH_CACHE_SIZE)],
    ok.

tanh_cached(X) when X >= -5.0, X =< 5.0 ->
    %% Use cache for common range
    Key = round(X * 100) / 100,
    case ets:lookup(tanh_cache, Key) of
        [{Key, Val}] -> Val;
        [] -> math:tanh(X)
    end;
tanh_cached(X) ->
    math:tanh(X).
```

### 3. Optimize Weight Perturbation

**Current Issues**:
- Random number generation overhead
- Multiple list traversals

**Optimizations**:
```erlang
%% Optimization 1: Generate random batch
%% Before: call rand:uniform() per weight
perturb_weights_old(Weights, Spread) ->
    [perturb_weight(W, Spread) || W <- Weights].

%% After: generate random values in batch
perturb_weights(Weights, Spread) ->
    N = length(Weights),
    Randoms = [rand:uniform() - 0.5 || _ <- lists:seq(1, N)],
    lists:zipwith(
        fun({W, DW, LP, LPs}, R) ->
            NewDW = R * Spread + DW * 0.5,
            {functions:sat(W + NewDW, ?SAT_LIMIT), NewDW, LP, LPs}
        end,
        Weights,
        Randoms
    ).

%% Optimization 2: Use binary for large weight vectors
%% Store weights as binary for memory efficiency
-spec weights_to_binary([weight_spec()]) -> binary().
weights_to_binary(Weights) ->
    << <<W:64/float, DW:64/float, LP:64/float>>
       || {W, DW, LP, _} <- Weights >>.
```

### 4. Optimize Mnesia Access

**Current Issues**:
- Synchronous reads in hot paths
- Transaction overhead
- No caching

**Optimizations**:
```erlang
%% Optimization 1: Use dirty reads where safe
%% Before: transaction
read_neuron_old(NeuronId) ->
    mnesia:transaction(fun() ->
        mnesia:read({neuron, NeuronId})
    end).

%% After: dirty read (faster, acceptable for reads)
read_neuron(NeuronId) ->
    case mnesia:dirty_read({neuron, NeuronId}) of
        [Neuron] -> {ok, Neuron};
        [] -> {error, not_found}
    end.

%% Optimization 2: Batch reads
read_neurons(NeuronIds) ->
    [read_neuron(Id) || Id <- NeuronIds].

%% Optimization 3: Cache genotype during evaluation
%% Load genotype once, keep in process state
cache_genotype(AgentId) ->
    Agent = genotype:read_agent(AgentId),
    Neurons = genotype:read_neurons(AgentId),
    Sensors = genotype:read_sensors(AgentId),
    #{
        agent => Agent,
        neurons => maps:from_list([{N#neuron.id, N} || N <- Neurons]),
        sensors => maps:from_list([{S#sensor.id, S} || S <- Sensors])
    }.
```

### 5. Remove Dead Code

**Tasks**:

1. **Remove commented-out code**
   - functions.erl: remove commented functions
   - Any module with large commented sections

2. **Remove unused record fields**
   ```erlang
   %% In neuron.erl
   %% Before: unused fields
   -record(state, {
       ...,
       si_pids=[],              % UNUSED - remove
       pre_processor,           % UNUSED - remove
       signal_integrator,       % UNUSED - remove
       post_processor,          % UNUSED - remove
       ...
   }).

   %% After: clean record
   -record(neuron_state, {
       ...,
       %% Only used fields
   }).
   ```

3. **Remove unused exports**
   - Audit all module exports
   - Remove functions only exported but never called

4. **Clean up macros**
   - Remove unused -define macros
   - Document remaining macros

### 6. Add Performance Benchmarks

```erlang
-module(tweann_benchmark).
-export([run_all/0, benchmark_forward/0, benchmark_mutation/0]).

%% Run all benchmarks
run_all() ->
    Results = [
        {forward_propagation, benchmark_forward()},
        {weight_perturbation, benchmark_perturbation()},
        {mutation_operators, benchmark_mutation()},
        {mnesia_access, benchmark_mnesia()}
    ],
    print_results(Results).

%% Benchmark forward propagation
benchmark_forward() ->
    %% Setup
    Network = test_helpers:create_benchmark_network(100),  % 100 neurons

    %% Warm up
    [run_forward_pass(Network) || _ <- lists:seq(1, 100)],

    %% Benchmark
    {Time, _} = timer:tc(fun() ->
        [run_forward_pass(Network) || _ <- lists:seq(1, 1000)]
    end),

    #{
        total_ms => Time / 1000,
        per_pass_us => Time / 1000,
        passes => 1000
    }.

%% Benchmark mutation
benchmark_mutation() ->
    {ok, AgentId} = test_helpers:create_benchmark_agent(),

    {Time, _} = timer:tc(fun() ->
        [genome_mutator:mutate_weights(AgentId)
         || _ <- lists:seq(1, 100)]
    end),

    test_helpers:cleanup_test_agent(AgentId),

    #{
        total_ms => Time / 1000,
        per_mutation_us => Time / 100,
        mutations => 100
    }.
```

## Tests to Write

### performance_test.erl

```erlang
-module(performance_test).
-include_lib("eunit/include/eunit.hrl").

%% ============================================================================
%% Performance regression tests
%% ============================================================================

forward_propagation_performance_test() ->
    %% Ensure forward pass completes in acceptable time
    Network = test_helpers:create_test_network(50),
    MaxTime = 1000,  % microseconds

    {Time, _Result} = timer:tc(fun() ->
        run_forward_pass(Network)
    end),

    ?assert(Time < MaxTime).

weight_perturbation_performance_test() ->
    %% Ensure perturbation is fast enough
    Weights = [create_weight_spec() || _ <- lists:seq(1, 1000)],
    MaxTime = 1000,  % microseconds

    {Time, _Result} = timer:tc(fun() ->
        perturbation_utils:perturb_weights(Weights, 0.1)
    end),

    ?assert(Time < MaxTime).

mnesia_read_performance_test() ->
    %% Dirty read should be fast
    {ok, AgentId} = test_helpers:create_test_agent_in_db(),
    NeuronIds = genotype:get_neuron_ids(AgentId),
    MaxTime = 100,  % microseconds per neuron

    {Time, _} = timer:tc(fun() ->
        [genotype:read_neuron(Id) || Id <- NeuronIds]
    end),

    PerNeuron = Time / length(NeuronIds),
    ?assert(PerNeuron < MaxTime),

    test_helpers:cleanup_test_agent(AgentId).

%% ============================================================================
%% Memory tests
%% ============================================================================

neuron_memory_usage_test() ->
    %% Ensure neuron state doesn't grow unboundedly
    InitialMemory = erlang:memory(processes),

    %% Create many neurons
    Pids = [spawn(fun neuron_test_loop/0) || _ <- lists:seq(1, 100)],

    FinalMemory = erlang:memory(processes),
    MemoryPerNeuron = (FinalMemory - InitialMemory) / 100,

    %% Cleanup
    [Pid ! stop || Pid <- Pids],

    %% Should be reasonable (< 10KB per neuron)
    ?assert(MemoryPerNeuron < 10240).

neuron_test_loop() ->
    receive stop -> ok end.
```

### optimization_verification_test.erl

```erlang
-module(optimization_verification_test).
-include_lib("eunit/include/eunit.hrl").

%% ============================================================================
%% Verify optimizations maintain correctness
%% ============================================================================

dot_product_optimization_correctness_test() ->
    %% Old and new implementation should produce same results
    Weights = [{0.5, 0.0, 0.1, []}, {0.3, 0.0, 0.1, []}],
    Inputs = [0.8, 0.6],

    %% Results should match
    OldResult = signal_aggregator:dot_product_reference(Weights, Inputs),
    NewResult = signal_aggregator:dot_product(Weights, Inputs),

    ?assert(abs(OldResult - NewResult) < 0.0001).

perturbation_optimization_correctness_test() ->
    %% Optimized perturbation should still perturb
    Weights = [{0.5, 0.0, 0.1, []} || _ <- lists:seq(1, 10)],
    Spread = 1.0,

    Perturbed = perturbation_utils:perturb_weights(Weights, Spread),

    %% Should be different
    ?assertNotEqual(Weights, Perturbed),
    %% Should have same structure
    ?assertEqual(length(Weights), length(Perturbed)).

cached_tanh_accuracy_test() ->
    %% Cached tanh should be accurate
    TestValues = [0.0, 0.5, 1.0, -0.5, -1.0, 2.5],

    lists:foreach(
        fun(X) ->
            Expected = math:tanh(X),
            Actual = functions:tanh_cached(X),
            ?assert(abs(Expected - Actual) < 0.01)
        end,
        TestValues
    ).
```

## Documentation Requirements

### Required Documentation

1. **Performance characteristics**
   - Hot path descriptions
   - Expected performance metrics
   - Benchmark results

2. **Optimization techniques**
   - Each optimization documented
   - Trade-offs explained

3. **Configuration guide**
   - Cache sizes
   - Batch sizes

### Documentation Checklist

- [ ] Hot paths identified and documented
- [ ] Optimizations explained
- [ ] Benchmark usage documented
- [ ] Configuration options listed

## Quality Gates

### v0.9.0 Acceptance Criteria

1. **Performance**
   - [ ] Forward propagation < 1ms for 100 neurons
   - [ ] Weight perturbation < 1ms for 1000 weights
   - [ ] Mnesia read < 100us per record

2. **Dead Code Removal**
   - [ ] No commented-out code
   - [ ] No unused record fields
   - [ ] No unused exports

3. **Benchmarks**
   - [ ] Benchmark suite complete
   - [ ] Baseline metrics recorded
   - [ ] No performance regressions

4. **Optimization Correctness**
   - [ ] All optimizations verified
   - [ ] Results match reference implementation
   - [ ] All tests pass

5. **Static Analysis**
   - [ ] Zero dialyzer warnings

## Known Limitations

- Some optimizations are micro-optimizations
- Cache requires memory overhead
- Dirty reads sacrifice consistency guarantees

## Next Steps

After v0.9.0 completion:

1. **v1.0.0** will finalize documentation and declare production readiness
2. Performance Phase complete
3. Full release validation

## Implementation Notes

### Profiling Workflow

1. Run baseline profiling
2. Identify top 5 hotspots
3. Implement optimization
4. Re-profile to verify improvement
5. Run correctness tests
6. Update benchmarks

### Cache Configuration

```erlang
%% In app configuration
{tweann, [
    {tanh_cache_size, 1000},
    {genotype_cache_ttl, 60000},  % 60 seconds
    {batch_read_size, 100}
]}
```

### Optimization Priority Order

1. **High Impact**: Signal aggregation, forward pass
2. **Medium Impact**: Weight perturbation, mutation
3. **Low Impact**: Logging, error handling

### Investigation: Nx Integration for Numerical Performance

**Objective**: Evaluate feasibility and benefits of integrating Elixir's Nx library (Numerical Elixir) for high-performance numerical computations.

**Background**:
- Nx provides tensor operations with optional GPU/CPU acceleration via backends
- Uses EXLA (XLA compiler) or Torchx (LibTorch) for compiled numerical operations
- May offer significant speedup for vector/matrix operations in neural networks

**Key Questions to Answer**:

1. **Compatibility**:
   - Can Erlang code call Nx NIFs directly?
   - Is Elixir dependency acceptable for pure Erlang library?
   - Interop overhead vs performance gain

2. **Performance Targets**:
   - Dot product computation (signal aggregation)
   - Weight vector operations (perturbation, mutation)
   - Activation function batching
   - Matrix operations for substrate networks (HyperNEAT)

3. **Implementation Approaches**:

   **Option A: Optional Elixir Dependency**
   ```erlang
   %% rebar.config
   {deps, [
       {nx, {git, "https://github.com/elixir-nx/nx.git", {tag, "v0.7.0"}}},
       {exla, {git, "https://github.com/elixir-nx/nx.git", {tag, "v0.7.0"}}}
   ]}.

   %% Use Nx if available, fallback to pure Erlang
   -ifdef(NX_AVAILABLE).
   dot_product(Weights, Inputs) ->
       nx_ops:dot_product(Weights, Inputs).
   -else.
   dot_product(Weights, Inputs) ->
       lists:sum([W * I || {W, I} <- lists:zip(Weights, Inputs)]).
   -endif.
   ```

   **Option B: Separate Acceleration Module**
   ```erlang
   %% src/tweann_accelerator.erl (optional module)
   -module(tweann_accelerator).
   -export([dot_product/2, batch_activate/2]).

   %% Use Nx tensors for bulk operations
   dot_product(WeightTensor, InputTensor) ->
       Nx.dot(WeightTensor, InputTensor).

   batch_activate(Signals, ActivationFn) ->
       Nx.map(Signals, activation_to_nx_fn(ActivationFn)).
   ```

   **Option C: Pure NIF Implementation**
   ```erlang
   %% Custom NIF using Rustler or C
   %% No Elixir dependency, maximum performance
   -module(tweann_nif).
   -on_load(init/0).

   init() ->
       SoName = filename:join(priv_dir(), "tweann_nif"),
       erlang:load_nif(SoName, 0).

   %% Implemented in Rust/C
   dot_product(_Weights, _Inputs) ->
       erlang:nif_error(not_loaded).
   ```

4. **Benchmarking Plan**:
   - Profile current pure Erlang performance
   - Implement Nx prototype for dot product
   - Measure overhead: data conversion, NIF calls
   - Compare: Small vectors (10), Medium (100), Large (1000)
   - Measure on: CPU only, GPU (if available)

5. **Decision Criteria**:
   - **Adopt Nx if**: >3x speedup for common operations, minimal overhead
   - **Custom NIF if**: Nx overhead too high, need fine control
   - **Stay Pure Erlang if**: Gains <2x, added complexity not justified

**Expected Outcomes**:

1. Performance comparison report
2. Recommendation: Nx, custom NIF, or pure Erlang
3. If beneficial: prototype implementation
4. If not: document why and alternative optimizations

**Investigation Effort**: 3-4 days
- Day 1: Nx integration spike, basic benchmarks
- Day 2: Comprehensive performance testing
- Day 3: Prototype implementation (if promising)
- Day 4: Documentation and recommendation

**References**:
- Nx: https://github.com/elixir-nx/nx
- EXLA: https://github.com/elixir-nx/nx/tree/main/exla
- Rustler (for NIF alternative): https://github.com/rusterlium/rustler

## Dependencies

### External Dependencies

- fprof, eprof (profiling)
- timer (benchmarking)

### Internal Dependencies

- All previously refactored modules
- Robustness features (v0.8.0)

## Effort Estimate

| Task | Estimate |
|------|----------|
| Profiling setup | 1 day |
| Forward propagation optimization | 2 days |
| Perturbation optimization | 1 day |
| Mnesia optimization | 1.5 days |
| Dead code removal | 1 day |
| Benchmark suite | 1.5 days |
| Documentation | 1 day |
| Verification | 1 day |
| **Total** | **10 days** |

## Risks

| Risk | Mitigation |
|------|------------|
| Optimization breaks behavior | Verification tests |
| Premature optimization | Profile first |
| Cache invalidation bugs | TTL and clear mechanisms |

---

**Version**: 0.9.0
**Phase**: Performance
**Status**: Planned
