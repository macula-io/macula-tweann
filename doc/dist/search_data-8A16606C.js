searchData={"items":[{"type":"module","title":"functions","doc":"Activation and utility functions for neural computation. This module provides activation functions used by neurons to transform aggregated input signals into output signals, plus utility functions for saturation and scaling. Activation Functions Monotonic: -  tanh  - Hyperbolic tangent, smooth, range [-1, 1] -  sigmoid  - Logistic function, range [0, 1] -  sigmoid1  - Alternative sigmoid, range [-1, 1] -  linear  - Identity function (no transformation) Periodic: -  sin  - Sine function -  cos  - Cosine function Radial Basis: -  gaussian  - Bell curve, peaks at 0 -  multiquadric  - sqrt(x^2 + c) Threshold: -  sgn  - Sign function {-1, 0, 1} -  bin  - Binary threshold {0, 1} -  trinary  - Three-level output {-1, 0, 1} Other: -  absolute  - Absolute value -  quadratic  - Signed square -  sqrt  - Signed square root -  log  - Signed logarithm Utility Functions -  saturation/1,2  - Clamp values to prevent overflow -  sat/3  - Clamp to [min, max] range -  sat_dzone/5  - Saturation with dead zone -  scale/3,5  - Scale values between ranges","ref":"functions.html"},{"type":"function","title":"functions.absolute/1","doc":"Absolute value activation function","ref":"functions.html#absolute/1"},{"type":"function","title":"functions.avg/1","doc":"Calculate average of a list","ref":"functions.html#avg/1"},{"type":"function","title":"functions.bin/1","doc":"Binary threshold function Returns 1 for positive input, 0 otherwise.","ref":"functions.html#bin/1"},{"type":"function","title":"functions.cos/1","doc":"Cosine activation function Periodic function with range [-1, 1]. cos(0) = 1, phase-shifted from sine.","ref":"functions.html#cos/1"},{"type":"function","title":"functions.gaussian/1","doc":"Gaussian (radial basis) activation function Bell curve centered at 0, peaks at 1.0, decays towards 0. Mathematical definition: e^(-x^2) Input is clamped to [-10, 10] to prevent underflow. Properties: - Output range: (0, 1] - gaussian(0) = 1 - Radially symmetric","ref":"functions.html#gaussian/1"},{"type":"function","title":"functions.gaussian/2","doc":"Gaussian with custom base","ref":"functions.html#gaussian/2"},{"type":"function","title":"functions.linear/1","doc":"Linear (identity) activation function No transformation - output equals input. Used for output neurons when raw values are needed.","ref":"functions.html#linear/1"},{"type":"function","title":"functions.log/1","doc":"Logarithm activation function Signed logarithm: sgn(x) * ln(|x|) Handles zero input specially.","ref":"functions.html#log/1"},{"type":"function","title":"functions.multiquadric/1","doc":"Multiquadric activation function Mathematical definition: sqrt(x^2 + 0.01) Always positive, smooth at origin.","ref":"functions.html#multiquadric/1"},{"type":"function","title":"functions.quadratic/1","doc":"Quadratic activation function Signed square: sgn(x) * x^2 Preserves sign while amplifying magnitude.","ref":"functions.html#quadratic/1"},{"type":"function","title":"functions.sat/3","doc":"Clamp value to range [Min, Max]","ref":"functions.html#sat/3"},{"type":"function","title":"functions.sat_dzone/5","doc":"Clamp value with dead zone Values within the dead zone [DZMin, DZMax] are set to 0. Values outside are clamped to [Min, Max].","ref":"functions.html#sat_dzone/5"},{"type":"function","title":"functions.saturation/1","doc":"Clamp value to default range [-1000, 1000] Prevents numerical overflow in subsequent calculations.","ref":"functions.html#saturation/1"},{"type":"function","title":"functions.saturation/2","doc":"Clamp value to symmetric range [-Spread, Spread]","ref":"functions.html#saturation/2"},{"type":"function","title":"functions.scale/3","doc":"Scale list or value from one range to [-1, 1] Normalizes values using: (Val*2 - (Max + Min)) / (Max - Min)","ref":"functions.html#scale/3"},{"type":"function","title":"functions.scale/5","doc":"Scale value from one range to another Linear interpolation from [FromMin, FromMax] to [ToMin, ToMax].","ref":"functions.html#scale/5"},{"type":"function","title":"functions.sgn/1","doc":"Sign function Returns the sign of the input value.","ref":"functions.html#sgn/1"},{"type":"function","title":"functions.sigmoid1/1","doc":"Alternative sigmoid activation function Maps to range [-1, 1] using: x / (1 + |x|) Properties: - Output range: (-1, 1) - sigmoid1(0) = 0 - Faster to compute than standard sigmoid - Derivative: 1 / (1 + |x|)^2","ref":"functions.html#sigmoid1/1"},{"type":"function","title":"functions.sigmoid/1","doc":"Sigmoid (logistic) activation function S-shaped curve mapping to range [0, 1]. Mathematical definition: 1 / (1 + e^-x) Input is clamped to [-10, 10] to prevent overflow. Properties: - Output range: (0, 1) - sigmoid(0) = 0.5 - Derivative: y * (1 - y)","ref":"functions.html#sigmoid/1"},{"type":"function","title":"functions.sin/1","doc":"Sine activation function Periodic function with range [-1, 1]. Useful for oscillatory patterns and fourier-like representations.","ref":"functions.html#sin/1"},{"type":"function","title":"functions.sqrt/1","doc":"Square root activation function Signed square root: sgn(x) * sqrt(|x|) Compresses magnitude while preserving sign.","ref":"functions.html#sqrt/1"},{"type":"function","title":"functions.std/1","doc":"Calculate standard deviation of a list","ref":"functions.html#std/1"},{"type":"function","title":"functions.tanh/1","doc":"Hyperbolic tangent activation function Maps input to range [-1, 1] with smooth gradient. Mathematical definition: tanh(x) = (e^x - e^-x) / (e^x + e^-x) Properties: - Output range: [-1, 1] - tanh(0) = 0 - Smooth derivative (good for learning) - Most commonly used activation in neuroevolution","ref":"functions.html#tanh/1"},{"type":"function","title":"functions.trinary/1","doc":"Trinary threshold function Returns -1, 0, or 1 based on threshold of 0.33.","ref":"functions.html#trinary/1"},{"type":"module","title":"signal_aggregator","doc":"Signal aggregation functions for neural computation. This module provides functions that aggregate weighted inputs from multiple sources into a single scalar value for activation function processing. Aggregation Methods -  dot_product  - Standard weighted sum (most common) -  mult_product  - Multiplicative aggregation -  diff_product  - Differentiation-based aggregation (uses process dictionary) Weight Tuple Format Weights are provided as tuples:  {Weight, DeltaWeight, LearningRate, ParamList} - Weight: The actual weight value used for computation - DeltaWeight: Momentum term (ignored here, used by plasticity) - LearningRate: Learning parameter (ignored here) - ParamList: Additional parameters for plasticity rules (ignored here) Only the Weight value is used for aggregation. The other fields support the plasticity system for weight updates during learning.","ref":"signal_aggregator.html"},{"type":"type","title":"signal_aggregator.actuator_id/0","doc":"","ref":"signal_aggregator.html#t:actuator_id/0"},{"type":"type","title":"signal_aggregator.cortex_id/0","doc":"","ref":"signal_aggregator.html#t:cortex_id/0"},{"type":"type","title":"signal_aggregator.delta_weight/0","doc":"","ref":"signal_aggregator.html#t:delta_weight/0"},{"type":"function","title":"signal_aggregator.diff_product/2","doc":"Compute differentiation-based product of inputs Uses the difference between current and previous inputs, then applies dot product aggregation. This implements temporal differentiation for detecting changes in input signals. Warning: This function uses the process dictionary to store previous input state. On first call, behaves like regular dot_product.","ref":"signal_aggregator.html#diff_product/2"},{"type":"function","title":"signal_aggregator.dot_product/2","doc":"Compute dot product of inputs and weights For each input source, multiplies each input signal component by its corresponding weight and sums all results. This is the standard weighted sum aggregation used in most neural networks. The bias term is handled specially - if present as the last weight entry with source ID 'bias', its weight is added directly to the result. Weight tuple format: {W, DW, LP, LPs} W - Weight value (used for computation) DW - Delta weight (ignored here, used by plasticity) LP - Learning parameter (ignored here) LPs - Parameter list (ignored here)","ref":"signal_aggregator.html#dot_product/2"},{"type":"type","title":"signal_aggregator.element_id/0","doc":"","ref":"signal_aggregator.html#t:element_id/0"},{"type":"type","title":"signal_aggregator.input_signal/0","doc":"","ref":"signal_aggregator.html#t:input_signal/0"},{"type":"type","title":"signal_aggregator.input_signals/0","doc":"","ref":"signal_aggregator.html#t:input_signals/0"},{"type":"type","title":"signal_aggregator.learning_rate/0","doc":"","ref":"signal_aggregator.html#t:learning_rate/0"},{"type":"function","title":"signal_aggregator.mult_product/2","doc":"Compute multiplicative product of inputs and weights For each input source, multiplies each input signal component by its corresponding weight, then multiplies all these products together. Useful for AND-like logic in neural networks. Note: Any zero input will result in zero output due to multiplication.","ref":"signal_aggregator.html#mult_product/2"},{"type":"type","title":"signal_aggregator.neuron_id/0","doc":"","ref":"signal_aggregator.html#t:neuron_id/0"},{"type":"type","title":"signal_aggregator.parameter_list/0","doc":"","ref":"signal_aggregator.html#t:parameter_list/0"},{"type":"type","title":"signal_aggregator.sensor_id/0","doc":"","ref":"signal_aggregator.html#t:sensor_id/0"},{"type":"type","title":"signal_aggregator.unique_id/0","doc":"","ref":"signal_aggregator.html#t:unique_id/0"},{"type":"type","title":"signal_aggregator.weight/0","doc":"","ref":"signal_aggregator.html#t:weight/0"},{"type":"type","title":"signal_aggregator.weight_list/0","doc":"","ref":"signal_aggregator.html#t:weight_list/0"},{"type":"type","title":"signal_aggregator.weight_spec/0","doc":"","ref":"signal_aggregator.html#t:weight_spec/0"},{"type":"type","title":"signal_aggregator.weighted_input/0","doc":"","ref":"signal_aggregator.html#t:weighted_input/0"},{"type":"type","title":"signal_aggregator.weighted_inputs/0","doc":"","ref":"signal_aggregator.html#t:weighted_inputs/0"},{"type":"extras","title":"Overview","doc":"# macula-tweann\nMacula TWEANN is a progression of DXNN2 by Gene Sher, bbased on his \"Handbook of Neuroevolution through Erlang\"","ref":"readme.html"}],"proglang":"erlang","content_type":"text/markdown","producer":{"name":"ex_doc","version":"0.38.2"}}