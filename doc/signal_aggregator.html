<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Module signal_aggregator</title>
<link rel="stylesheet" type="text/css" href="stylesheet.css" title="EDoc">
</head>
<body bgcolor="white">
<div class="navbar"><a name="#navbar_top"></a><table width="100%" border="0" cellspacing="0" cellpadding="2" summary="navigation bar"><tr><td><a href="overview-summary.html" target="overviewFrame">Overview</a></td><td><a href="http://www.erlang.org/"><img src="erlang.png" align="right" border="0" alt="erlang logo"></a></td></tr></table></div>
<hr>

<h1>Module signal_aggregator</h1>
<ul class="index"><li><a href="#description">Description</a></li><li><a href="#types">Data Types</a></li><li><a href="#index">Function Index</a></li><li><a href="#functions">Function Details</a></li></ul>Signal aggregation functions for neural computation.
<p>Copyright Â© 2025 Macula.io, Apache-2.0</p>

<p><b>Authors:</b> Macula.io.</p>

<h2><a name="description">Description</a></h2><p>Signal aggregation functions for neural computation.</p>
 
  <p>This module provides functions that aggregate weighted inputs from multiple  
sources into a single scalar value for activation function processing.</p>
 
  <h3><a name="Aggregation_Methods">Aggregation Methods</a></h3>
 
  <p>- <code>dot_product</code> - Standard weighted sum (most common)</p>
 
  <p>- <code>mult_product</code> - Multiplicative aggregation</p>
 
  <p>- <code>diff_product</code> - Differentiation-based aggregation (uses process dictionary)</p>
 
  <h3><a name="Weight_Tuple_Format">Weight Tuple Format</a></h3>
 
  <p>Weights are provided as tuples: <code>{Weight, DeltaWeight, LearningRate, ParamList}</code></p>
 
  <p>- Weight: The actual weight value used for computation</p>
 
  <p>- DeltaWeight: Momentum term (ignored here, used by plasticity)</p>
 
  <p>- LearningRate: Learning parameter (ignored here)</p>
 
  <p>- ParamList: Additional parameters for plasticity rules (ignored here)</p>
 
  Only the Weight value is used for aggregation. The other fields support
  the plasticity system for weight updates during learning.
 
<h2><a name="types">Data Types</a></h2>

<h3 class="typedecl"><a name="type-input_signal">input_signal()</a></h3>
<p><code>input_signal() = {<a href="#type-element_id" docgen-rel="seetype" docgen-href="#element_id/0">element_id()</a>, [float()]}</code></p>


<h3 class="typedecl"><a name="type-input_signals">input_signals()</a></h3>
<p><code>input_signals() = [<a href="#type-input_signal" docgen-rel="seetype" docgen-href="#input_signal/0">input_signal()</a>]</code></p>


<h2><a name="index">Function Index</a></h2>
<table width="100%" border="1" cellspacing="0" cellpadding="2" summary="function index"><tr><td valign="top"><a href="#diff_product-2">diff_product/2</a></td><td>Compute differentiation-based product of inputs.</td></tr>
<tr><td valign="top"><a href="#dot_product-2">dot_product/2</a></td><td>Compute dot product of inputs and weights.</td></tr>
<tr><td valign="top"><a href="#dot_product_nif-2">dot_product_nif/2</a></td><td>NIF-accelerated dot product when available.</td></tr>
<tr><td valign="top"><a href="#flatten_for_nif-2">flatten_for_nif/2</a></td><td>Flatten nested signal/weight structure for NIF consumption.</td></tr>
<tr><td valign="top"><a href="#mult_product-2">mult_product/2</a></td><td>Compute multiplicative product of inputs and weights.</td></tr>
</table>

<h2><a name="functions">Function Details</a></h2>

<h3 class="function"><a name="diff_product-2">diff_product/2</a></h3>
<div class="spec">
<p><code>diff_product(InputSignals::<a href="#type-input_signals" docgen-rel="seetype" docgen-href="#input_signals/0">input_signals()</a>, WeightedInputs::<a href="#type-weighted_inputs" docgen-rel="seetype" docgen-href="#weighted_inputs/0">weighted_inputs()</a>) -&gt; float()</code><br></p>
<p><code>InputSignals</code>: List of {SourceId, SignalVector} tuples<br>
<code>WeightedInputs</code>: List of {SourceId, [WeightSpec]} tuples<br>
</p>
<p>returns: Aggregated scalar value based on input differences</p>
</div><p><p>Compute differentiation-based product of inputs</p>
 
  <p>Uses the difference between current and previous inputs, then applies  
dot product aggregation. This implements temporal differentiation for  
detecting changes in input signals.</p>
 
  Warning: This function uses the process dictionary to store previous
  input state. On first call, behaves like regular dot_product.
 </p>

<h3 class="function"><a name="dot_product-2">dot_product/2</a></h3>
<div class="spec">
<p><code>dot_product(InputSignals::<a href="#type-input_signals" docgen-rel="seetype" docgen-href="#input_signals/0">input_signals()</a>, WeightedInputs::<a href="#type-weighted_inputs" docgen-rel="seetype" docgen-href="#weighted_inputs/0">weighted_inputs()</a>) -&gt; float()</code><br></p>
<p><code>InputSignals</code>: List of {SourceId, SignalVector} tuples<br>
<code>WeightedInputs</code>: List of {SourceId, [WeightSpec]} tuples<br>
</p>
<p>returns: <p>Aggregated scalar value</p>
 
  <p>Example:</p>
 
  Inputs = [{sensor1, [1.0, 0.5]}],
  Weights = [{sensor1, [{0.3, 0.0, 0.1, []}, {0.7, 0.0, 0.1, []}]}],
  Result = dot_product(Inputs, Weights).
  Result = 1.0*0.3 + 0.5*0.7 = 0.65</p>
</div><p><p>Compute dot product of inputs and weights</p>
 
  <p>For each input source, multiplies each input signal component by its  
corresponding weight and sums all results. This is the standard weighted  
sum aggregation used in most neural networks.</p>
 
  <p>The bias term is handled specially - if present as the last weight entry  
with source ID 'bias', its weight is added directly to the result.</p>
 
  Weight tuple format: {W, DW, LP, LPs}
    W  - Weight value (used for computation)
    DW - Delta weight (ignored here, used by plasticity)
    LP - Learning parameter (ignored here)
    LPs - Parameter list (ignored here)
 </p>

<h3 class="function"><a name="dot_product_nif-2">dot_product_nif/2</a></h3>
<div class="spec">
<p><code>dot_product_nif(InputSignals::<a href="#type-input_signals" docgen-rel="seetype" docgen-href="#input_signals/0">input_signals()</a>, WeightedInputs::<a href="#type-weighted_inputs" docgen-rel="seetype" docgen-href="#weighted_inputs/0">weighted_inputs()</a>) -&gt; float()</code><br></p>
<p><code>InputSignals</code>: List of {SourceId, SignalVector} tuples<br>
<code>WeightedInputs</code>: List of {SourceId, [WeightSpec]} tuples<br>
</p>
<p>returns: Aggregated scalar value</p>
</div><p><p>NIF-accelerated dot product when available.</p>
 
  <p>This function attempts to use the Rust NIF for dot product computation.  
Falls back to pure Erlang if NIF is not loaded.</p>
 
  Performance: 40-100x faster than pure Erlang for N &gt; 10 inputs.
 </p>

<h3 class="function"><a name="flatten_for_nif-2">flatten_for_nif/2</a></h3>
<div class="spec">
<p><code>flatten_for_nif(InputSignals::<a href="#type-input_signals" docgen-rel="seetype" docgen-href="#input_signals/0">input_signals()</a>, WeightedInputs::<a href="#type-weighted_inputs" docgen-rel="seetype" docgen-href="#weighted_inputs/0">weighted_inputs()</a>) -&gt; {[float()], [float()], float()}</code><br></p>
<p><code>InputSignals</code>: List of {SourceId, SignalVector} tuples<br>
<code>WeightedInputs</code>: List of {SourceId, [WeightSpec]} tuples<br>
</p>
<p>returns: {FlatSignals, FlatWeights, Bias}</p>
</div><p><p>Flatten nested signal/weight structure for NIF consumption.</p>
 
  <p>Converts from:    
Signals: [{SourceId, [S1, S2, ...]}, ...]    
Weights: [{SourceId, [{W1, DW1, LP1, []}, {W2, DW2, LP2, []}, ...]}, ...]</p>
 
  <p>To:    
FlatSignals: [S1, S2, S3, ...]    
FlatWeights: [W1, W2, W3, ...]    
Bias: float()</p>
 
  The flattened format is cache-friendly and suitable for SIMD vectorization
  in the Rust NIF.
 </p>

<h3 class="function"><a name="mult_product-2">mult_product/2</a></h3>
<div class="spec">
<p><code>mult_product(InputSignals::<a href="#type-input_signals" docgen-rel="seetype" docgen-href="#input_signals/0">input_signals()</a>, WeightedInputs::<a href="#type-weighted_inputs" docgen-rel="seetype" docgen-href="#weighted_inputs/0">weighted_inputs()</a>) -&gt; float()</code><br></p>
<p><code>InputSignals</code>: List of {SourceId, SignalVector} tuples<br>
<code>WeightedInputs</code>: List of {SourceId, [WeightSpec]} tuples<br>
</p>
<p>returns: <p>Aggregated scalar value</p>
 
  <p>Example:</p>
 
  Inputs = [{sensor1, [0.5, 0.4]}],
  Weights = [{sensor1, [{2.0, 0.0, 0.1, []}, {3.0, 0.0, 0.1, []}]}],
  Result = mult_product(Inputs, Weights).
  Result = (0.5*2.0) * (0.4*3.0) = 1.2</p>
</div><p><p>Compute multiplicative product of inputs and weights</p>
 
  <p>For each input source, multiplies each input signal component by its  
corresponding weight, then multiplies all these products together.  
Useful for AND-like logic in neural networks.</p>
 
  Note: Any zero input will result in zero output due to multiplication.
 </p>
<hr>

<div class="navbar"><a name="#navbar_bottom"></a><table width="100%" border="0" cellspacing="0" cellpadding="2" summary="navigation bar"><tr><td><a href="overview-summary.html" target="overviewFrame">Overview</a></td><td><a href="http://www.erlang.org/"><img src="erlang.png" align="right" border="0" alt="erlang logo"></a></td></tr></table></div>
<p><i>Generated by EDoc</i></p>
</body>
</html>
